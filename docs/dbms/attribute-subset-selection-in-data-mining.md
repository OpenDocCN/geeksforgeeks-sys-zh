# 数据挖掘中的属性子集选择

> 原文:[https://www . geesforgeks . org/属性-子集-数据挖掘中的选择/](https://www.geeksforgeeks.org/attribute-subset-selection-in-data-mining/)

属性子集选择是数据挖掘过程中用于数据约简的一种技术。数据缩减减少了数据的大小，因此可以更有效地用于分析目的。

**需要属性子集选择-**
数据集可能有大量属性。但是其中一些属性可能是不相关的或多余的。属性子集选择的目标是找到一组最小的属性，这样删除那些不相关的属性不会对数据的效用产生太大影响，并且可以降低数据分析的成本。对精简数据集的挖掘也使发现的模式更容易理解。

**属性子集选择的过程-**
强力方法可能非常昂贵，其中可以分析具有 *n* 属性的数据的每个子集(2^n 可能的子集)。
完成任务的最佳方法是使用统计显著性测试，以便可以识别最佳(或最差)属性。统计显著性检验假设属性相互独立。这是一种贪婪方法，其中确定显著性水平(显著性水平的统计理想值为 5%)，并且一次又一次地测试模型，直到所有属性的 p 值(概率值)小于或等于选定的显著性水平。p 值高于显著性水平的属性被丢弃。此过程反复重复，直到数据集中的所有属性的 p 值都小于或等于显著性水平。这给了我们没有不相关属性的精简数据集。

**属性子集选择方法-**
1。逐步向前选择。
2。逐步向后消除。
3。正向选择和反向淘汰相结合。
4。决策树归纳。

以上方法都是属性子集选择的贪婪方法。

1.  **逐步向前选择:**此过程以一组空属性作为最小集开始。选择最相关的属性(具有最小 p 值)并将其添加到最小集合中。在每次迭代中，一个属性被添加到一个缩减的集合中。
2.  **逐步向后淘汰:**这里所有的属性都考虑在初始属性集中。在每次迭代中，从 p 值高于显著性水平的属性集中删除一个属性。
3.  **向前选择和向后消除的组合:**将逐步向前选择和向后消除相结合，以便最有效地选择相关属性。这是最常见的技术，通常用于属性选择。
4.  **决策树归纳:**该方法使用决策树进行属性选择。它构建了一个类似流程图的结构，其中的节点表示对属性的测试。每个分支对应于测试的结果，叶节点是一个类预测。不是树的一部分的属性被认为是不相关的，因此被丢弃。